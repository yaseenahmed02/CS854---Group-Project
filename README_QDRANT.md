# Multimodal RAG with Qdrant for SWE-bench

This project implements a **Multimodal Retrieval-Augmented Generation (RAG)** system designed for software engineering tasks, specifically targeting the **SWE-bench Multimodal** dataset. It leverages **Qdrant** as the vector database to store and retrieve both code and visual information (from issue images).

## ðŸ—ï¸ Architecture

The system uses a dual-store approach within Qdrant:

1.  **Code Store (Per-Repository)**:
    *   **Structure**: Each repository version is stored in its own Qdrant collection (e.g., `grommet_grommet_2_10_0`).
    *   **Vectors**:
        *   **Dense**: `jinaai/jina-embeddings-v2-base-code` (8192 context).
        *   **Sparse**: `prithivida/Splade_PP_en_v1` (SPLADE) and `BAAI/bge-m3` (BGE-M3).
    *   **Storage**: Local Qdrant instances at `./qdrant_data_{repo}_{version}`.

2.  **Multimodal Store (Global)**:
    *   **Collection**: `swe_images`.
    *   **Content**: Textual descriptions of images found in GitHub issues, generated by a VLM (GPT-4o).
    *   **Vectors**: Dense embeddings (Jina-v2) of the descriptions.
    *   **Storage**: Local Qdrant instance at `./qdrant_data_swe_images`.

## ðŸš€ Getting Started

### Prerequisites
- Python 3.10+
- `pip install -r requirements.txt`
- OpenAI API Key (for VLM generation)
- vLLM (or compatible LLM server) running for code generation.

### 1. Ingestion

**Ingest Codebase:**
Chunks the repository, generates dense/sparse embeddings, and upserts to Qdrant.
```bash
python ingest_code_to_qdrant.py \
  --repo_path /path/to/repo \
  --repo_name owner/repo \
  --repo_version 1.0.0
```

**Ingest Images (Multimodal):**
Processes images from the SWE-bench dataset, generates descriptions using GPT-4o, and upserts to Qdrant.
```bash
python ingest_images_to_qdrant.py \
  --split test \
  --limit 100
```

### 2. Retrieval

The `FlexibleRetriever` supports dynamic strategy selection at query time.

**Supported Strategies:**
- `['bm25']`: Standard BM25 (built from Qdrant documents).
- `['jina']`: Dense retrieval.
- `['splade']` / `['bge']`: Sparse retrieval.
- `['jina', 'splade']`: Hybrid retrieval with Reciprocal Rank Fusion (RRF).

**Multimodal Modes:**
- `visual_mode='none'`: Text-only retrieval.
- `visual_mode='augment'`: Appends VLM descriptions of relevant images to the text query.
- `visual_mode='fusion'`: Performs separate code and visual queries, then fuses results.

### 3. Generation (RAG Pipeline)

The `RAGPipeline` integrates retrieval with an LLM (via vLLM) to generate patches.

- **Mode `code_gen`**:
    - System prompt instructs the LLM to rewrite the entire file.
    - Post-processing extracts the code block and computes a **unified diff** against the original file.

### 4. Benchmarking

Run the 9-way experimental study on SWE-bench Multimodal.

```bash
# Run 1 instance with mock LLM for testing
python benchmark/run_experiments.py --limit 1 --mock

# Run full benchmark (requires running vLLM server)
python benchmark/run_experiments.py
```

### 5. Measure Recall

To rigorously evaluate the retrieval performance (checking if the retrieved files match the actual modified files in the solution), run the recall analysis script.

```bash
python benchmark/measure_recall.py
```
This will output a summary table and save detailed statistics to `results/recall_analysis.csv`.

**Experiments (13-Way Study):**

The `benchmark/run_experiments.py` script executes the following 13 configurations. Each configuration generates a separate `results/{experiment_name}_predictions.json` file.

**1. Text-Only Baselines**
*   `text_bm25`: Standard BM25 (Keyword matching).
*   `text_splade`: Sparse Neural Retrieval (Learned keywords).
*   `text_bge`: Multilingual Sparse Retrieval (BGE-M3).
*   `text_jina`: Dense Retrieval (Semantic search).

**2. Text-Only Hybrid (RRF Fusion)**
*   `text_hybrid_jina_bm25`: Combines Jina (Dense) + BM25.
*   `text_hybrid_jina_splade`: Combines Jina (Dense) + SPLADE.
*   `text_hybrid_jina_bge`: Combines Jina (Dense) + BGE-M3.

**3. Visual-Only**
*   `visual_only_jina`: Retrieves code using *only* the VLM-generated technical description of the screenshot.

**4. Multimodal Fusion (Text + Visual)**
*   `multimodal_fusion_bm25`: Fuses BM25(Text) + BM25(Visual Description).
*   `multimodal_fusion_splade`: Fuses SPLADE(Text) + SPLADE(Visual Description).
*   `multimodal_fusion_bge`: Fuses BGE(Text) + BGE(Visual Description).
*   `multimodal_fusion_jina`: Fuses Jina(Text) + Jina(Visual Description).

**5. The "Kitchen Sink"** 
*   `multimodal_fusion_hybrid_jina_best_sparse`: Fuses Jina(Text) + Best Sparse(Text) + Jina(Visual) + Best Sparse(Visual).

---

### Comparative Analysis: How to Interpret Results

By comparing the output files, you can answer specific research questions:

**A. Sparse (Statistical and Neural) vs. Dense vs. Hybrid (Text-Only)**
*   **Compare**: `text_bm25` vs. `text_splade` vs. `text_bge` vs. `text_jina` vs. `text_hybrid_jina_bm25` vs. `text_hybrid_jina_splade` vs. `text_hybrid_jina_bge`.
*   **Goal**: Determine if semantic search (Jina) outperforms keyword search (BM25) or learned sparse searches (SPLADE or BGE-M3) for code issues.
*   **Hybrid Check**: Compare `text_jina` vs. `text_hybrid_jina_splade` or `text_hybrid_jina_bge`. Does adding sparse signals to dense vectors improve recall?

**B. The Value of Visual Context**
*   **Compare**: `text_jina` vs. `visual_only_jina` vs. `multimodal_fusion_jina`.
*   **Goal**: Does adding the VLM's analysis of the screenshot actually help find the right code?
*   **Visual Validity**: Check `visual_only_jina`. If this scores > 0, it proves the screenshots contain unique, solvable signal.

**C. Best Multimodal Strategy**
*   **Compare**: `multimodal_fusion_bm25` vs. `multimodal_fusion_splade` vs. `multimodal_fusion_bge` vs. `multimodal_fusion_jina`.
*   **Goal**: Is it better to match the VLM's description using keywords (BM25) or semantics (Jina)?

**D. System Max Performance**
*   **Check**: `multimodal_fusion_hybrid_jina_best_sparse`.
*   **Goal**: Does throwing every signal (Text, Visual, Dense, Sparse) at the problem yield the highest score?

## ðŸ“‚ Project Structure

- `embeddings/`: `EmbeddingGenerator` for Dense, SPLADE, and BGE-M3.
- `retrieval/`: `FlexibleRetriever` and `HybridRetriever` logic.
- `rag/`: `RAGPipeline` for orchestration and patch generation.
- `utils/`: Helpers for ingestion (`SemanticChunker`) and patch cleaning (`patch_cleaner.py`).
- `benchmark/`: Scripts to run experiments.
