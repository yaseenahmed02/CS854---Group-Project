# Multimodal RAG with Qdrant: Step-by-Step Walkthrough

This document details how the project implements **Multimodal RAG** using **Qdrant**, connecting the ingestion of visual data to the final generation of code patches.

## üèóÔ∏è High-Level Architecture

The system uses a **Dual-Store Qdrant Architecture**:
1.  **Code Store (Per-Repository)**: Contains code chunks indexed by Dense (Jina), Sparse (SPLADE/BGE), and BM25 vectors.
2.  **Multimodal Store (Global)**: Contains "technical descriptions" of screenshots from GitHub issues, generated by a Vision Language Model (VLM). It can also optionally store the raw image (base64) in the payload, though retrieval is performed on the description vectors.

---

## Phase 1: Ingestion (The Foundation)

Before any retrieval happens, data must be processed and indexed.

### Step 1.1: Image Ingestion
**File:** [`ingest_images_to_qdrant.py`](file:///Users/talesmp/Documents/Development/githubPersonal/CS854---Group-Project/ingest_images_to_qdrant.py)

This script bridges the gap between visual pixels and textual search.

1.  **Load Data**: Reads the `princeton-nlp/SWE-bench_Multimodal` dataset.
2.  **Extract Images**: Finds image URLs in the issue description.
3.  **VLM Analysis (Crucial Step)**:
    *   It sends the image + issue text to **GPT-4o**.
    *   **System Prompt**: *"You are a Senior Front-End Engineer... technically reverse-engineer it into a search query."*
    *   **Goal**: Convert "The button looks wrong" into "CSS class .btn-primary padding issue flex-box misalignment".
4.  **Embedding**: The generated *text description* is embedded using `jina-embeddings-v2-base-code`.
5.  **Upsert**: Stores the vector + payload (including the raw VLM description and optionally the base64 image) into the `swe_images` collection in Qdrant.

**Example Log:**
```text
Processing image for instance_id: astropy__astropy-1234...
> VLM Description: "Matplotlib plot axis label overlap. Keywords: set_xlabel, tight_layout, FigureCanvasAgg."
> Upserted to collection 'swe_images'.
```

---

## Phase 2: The Retrieval Pipeline (Runtime)

This is where the magic happens during benchmarking.

### Step 2.1: The Driver
**File:** [`benchmark/run_experiments.py`](file:///Users/talesmp/Documents/Development/githubPersonal/CS854---Group-Project/benchmark/run_experiments.py)

This script iterates through defined experiments (e.g., `multimodal_fusion_jina`) and orchestrates the flow.

1.  **Setup**: It loads the dataset and initializes the `FlexibleRetriever`.
2.  **Dynamic Configuration**: It patches the `RAGPipeline` to use a specific configuration for the current experiment.
    *   *Example Config*: `{"strategies": ["jina"], "visual_mode": "fusion"}`

### Step 2.2: The Flexible Retriever
**File:** [`retrieval/flexible_retriever.py`](file:///Users/talesmp/Documents/Development/githubPersonal/CS854---Group-Project/retrieval/flexible_retriever.py)

This is the brain of the operation. It decides *how* to search based on the configuration.

**Function:** `retrieve(query, instance_id, strategy, visual_mode)`

#### Path A: Text-Only (Baseline)
*   **Input**: `visual_mode="none"`
*   **Action**: Searches the Code Store using the text query (Issue Problem Statement).
*   **Strategies**: Can use `['bm25']`, `['splade']` (Sparse), `['bge']` (Sparse) or `['jina']` (Dense).

#### Path B: Multimodal Augmentation
*   **Input**: `visual_mode="augment"`
*   **Action**:
    1.  Fetches **all** VLM descriptions from `swe_images` using `instance_id`.
    2.  **Concatenates**: `New Query = "Issue Text" + " " + "VLM Description 1" + " " + "VLM Description 2" ...`
    3.  Searches Code Store with this enriched query.
*   **Logic**: The visual keywords act as expansion terms for the text query.

#### Path C: Multimodal Fusion (The "Kitchen Sink")
*   **Input**: `visual_mode="fusion"`
*   **Action**:
    1.  **Query 1 (Text)**: Runs the selected strategies (e.g., Jina) on the *Issue Text*.
    2.  **Query 2..N (Visual)**: Runs the *same* strategies on **each** *VLM Description* separately.
    3.  **Fusion**: Merges all result lists using **Reciprocal Rank Fusion (RRF)**.
*   **Logic**: This treats each visual description as a completely independent search signal. If an issue has 3 images, the system will fuse results from 1 Text Query + 3 Visual Queries.

**Example Log (Fusion Mode):**
```text
Running strategy: jina for query: "Fix the navigation bar overlap..."
> Retrieved 20 docs (Text Signal)
Running separate visual query for fusion
Running strategy: jina for query: "CSS z-index issue in Navbar component..."
> Retrieved 20 docs (Visual Signal)
Fusing rankings...
> Final Top 5: [Navbar.js, App.css, Header.js, ...]
```

---

## Phase 3: Generation (The RAG)

**File:** [`rag/pipeline.py`](file:///Users/talesmp/Documents/Development/githubPersonal/CS854---Group-Project/rag/pipeline.py)

Once the relevant code files are retrieved, the system attempts to fix the bug.

1.  **Context Building**: Takes the Top-K retrieved code chunks.
2.  **Prompt Construction**:
    *   **System**: "You are a Senior Software Engineer... Rewrite the entire file..."
    *   **User**: "Issue: [Problem] \n Target File: [Content of Top Retrieved File]"
3.  **LLM Call**: Sends the prompt to **vLLM** (hosting a model like DeepSeek-Coder or Llama-3).
4.  **Post-Processing**:
    *   Extracts the code block from the LLM response.
    *   Computes a `unified_diff` between the original file and the generated file.

**Result**: A patch file (e.g., `model_patch.diff`) that can be applied to the repository.

---

## ‚ö†Ô∏è Possible Missteps & Edge Cases

| Scenario | Symptom | Root Cause | Fix |
| :--- | :--- | :--- | :--- |
| **Missing Image DB** | `Warning: Image DB not found` | `ingest_images_to_qdrant.py` was not run. | Run image ingestion script. |
| **Zero Results** | `Retrieved 0 documents` | The repo version in the dataset doesn't match the ingested Qdrant collection name. | Check `get_collection_name` logic in `run_experiments.py`. |
| **BM25 Error** | `ValueError: No documents found for BM25` | `chunks.json` is missing or empty, and Qdrant scroll failed. | Ensure `ingest_code_to_qdrant.py` generated chunks correctly. |
| **Hallucinated Files** | LLM outputs code for a file that doesn't exist. | The retriever failed to find the relevant file, so the LLM "guessed" based on the filename in the prompt or context. | Improve retrieval recall (use Hybrid/Fusion). |
| **VLM "Refusal"** | Description is "I cannot analyze this image." | OpenAI safety filters or unclear image. | Check `ingest_images` logs for VLM failures. |

## üìä Example Results (JSON Output)

The final output in `results/{experiment_name}_predictions.json` looks like this:

```json
{
  "instance_id": "astropy__astropy-1234",
  "model_patch": "--- a/file.py\n+++ b/file.py\n@@ -10,2 +10,2 @@\n- x = 1\n+ x = 2",
  "metrics": {
    "retrieval_time_ms": 150.5,
    "generation_time_ms": 4500.2,
    "total_tokens": 1250
  },
  "retrieved_documents": [
    {
      "id": "uuid-1",
      "score": 0.85,
      "payload": { "path": "astropy/visualization/wcsaxes/core.py", ... }
    }
  ]
}
```
